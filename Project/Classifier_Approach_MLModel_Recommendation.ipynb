{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I6w7sWqr1S7L",
        "outputId": "0304b800-7ef2-4110-e6ed-33e5845d6ce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-08 17:27:36--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘winequality-red.csv’\n",
            "\n",
            "\rwinequality-red.csv     [<=>                 ]       0  --.-KB/s               \rwinequality-red.csv     [ <=>                ]  82.23K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-05-08 17:27:36 (1.63 MB/s) - ‘winequality-red.csv’ saved [84199]\n",
            "\n",
            "--2025-05-08 17:27:36--  https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘abalone.csv’\n",
            "\n",
            "abalone.csv             [ <=>                ] 187.38K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-05-08 17:27:36 (2.76 MB/s) - ‘abalone.csv’ saved [191873]\n",
            "\n",
            "--2025-05-08 17:27:36--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘iris.csv’\n",
            "\n",
            "iris.csv                [ <=>                ]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-08 17:27:36 (67.3 MB/s) - ‘iris.csv’ saved [4551]\n",
            "\n",
            "--2025-05-08 17:27:36--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23278 (23K) [text/plain]\n",
            "Saving to: ‘pima.csv’\n",
            "\n",
            "pima.csv            100%[===================>]  22.73K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2025-05-08 17:27:37 (7.96 MB/s) - ‘pima.csv’ saved [23278/23278]\n",
            "\n",
            "--2025-05-08 17:27:37--  https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘glass.csv’\n",
            "\n",
            "glass.csv               [ <=>                ]  11.62K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-08 17:27:37 (138 MB/s) - ‘glass.csv’ saved [11903]\n",
            "\n",
            "--2025-05-08 17:27:37--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 76466 (75K) [text/plain]\n",
            "Saving to: ‘ionosphere.csv’\n",
            "\n",
            "ionosphere.csv      100%[===================>]  74.67K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-08 17:27:37 (2.37 MB/s) - ‘ionosphere.csv’ saved [76466/76466]\n",
            "\n",
            "--2025-05-08 17:27:38--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/haberman.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3103 (3.0K) [text/plain]\n",
            "Saving to: ‘haberman.csv’\n",
            "\n",
            "haberman.csv        100%[===================>]   3.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-08 17:27:38 (37.1 MB/s) - ‘haberman.csv’ saved [3103/3103]\n",
            "\n",
            "--2025-05-08 17:27:38--  https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘balance.csv’\n",
            "\n",
            "balance.csv             [ <=>                ]   6.10K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-08 17:27:38 (94.5 MB/s) - ‘balance.csv’ saved [6250]\n",
            "\n",
            "--2025-05-08 17:27:38--  https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘car.csv’\n",
            "\n",
            "car.csv                 [ <=>                ]  50.65K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-08 17:27:38 (1.50 MB/s) - ‘car.csv’ saved [51867]\n",
            "\n",
            "--2025-05-08 17:27:38--  https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘seeds.csv’\n",
            "\n",
            "seeds.csv               [ <=>                ]   9.08K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-08 17:27:38 (83.3 MB/s) - ‘seeds.csv’ saved [9300]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# MLP (Tabular) Datasets\n",
        "!wget -O winequality-red.csv https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
        "!wget -O abalone.csv https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data\n",
        "!wget -O iris.csv https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "!wget -O pima.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\n",
        "!wget -O glass.csv https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data\n",
        "!wget -O ionosphere.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv\n",
        "!wget -O haberman.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/haberman.csv\n",
        "!wget -O balance.csv https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data\n",
        "!wget -O car.csv https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\n",
        "!wget -O seeds.csv https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN (Image) Datasets\n",
        "!wget -O mnist.npz https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
        "!wget -O fashion-mnist-images.gz https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/train-images-idx3-ubyte.gz\n",
        "!wget -O cifar10.tar.gz https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!wget -O cifar100.tar.gz https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!wget -O svhn.mat http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
        "!wget -O flowers102.tgz https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "!wget -O big.txt https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt\n",
        "!wget -O words.txt https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\n",
        "!wget -O tweet_emotion.csv https://raw.githubusercontent.com/dair-ai/emotion_dataset/master/data/text_emotion.csv\n",
        "!wget -O rt_reviews.tar.gz https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2EHFZn71-Tw",
        "outputId": "768f46f1-e25f-4645-ca54-ab8a3c682a36",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-08 17:27:42--  https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.164.27, 172.217.0.91, 172.217.7.59, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.164.27|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11490434 (11M) [application/octet-stream]\n",
            "Saving to: ‘mnist.npz’\n",
            "\n",
            "mnist.npz           100%[===================>]  10.96M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-05-08 17:27:43 (139 MB/s) - ‘mnist.npz’ saved [11490434/11490434]\n",
            "\n",
            "--2025-05-08 17:27:43--  https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/train-images-idx3-ubyte.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/train-images-idx3-ubyte.gz [following]\n",
            "--2025-05-08 17:27:43--  https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/data/fashion/train-images-idx3-ubyte.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26421880 (25M) [application/octet-stream]\n",
            "Saving to: ‘fashion-mnist-images.gz’\n",
            "\n",
            "fashion-mnist-image 100%[===================>]  25.20M  43.7MB/s    in 0.6s    \n",
            "\n",
            "2025-05-08 17:27:45 (43.7 MB/s) - ‘fashion-mnist-images.gz’ saved [26421880/26421880]\n",
            "\n",
            "--2025-05-08 17:27:45--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar10.tar.gz’\n",
            "\n",
            "cifar10.tar.gz      100%[===================>] 162.60M  47.4MB/s    in 3.9s    \n",
            "\n",
            "2025-05-08 17:27:49 (41.8 MB/s) - ‘cifar10.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "--2025-05-08 17:27:49--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar100.tar.gz’\n",
            "\n",
            "cifar100.tar.gz     100%[===================>] 161.17M  54.1MB/s    in 3.0s    \n",
            "\n",
            "2025-05-08 17:27:52 (54.1 MB/s) - ‘cifar100.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "--2025-05-08 17:27:52--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
            "Resolving ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
            "Connecting to ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182040794 (174M) [text/plain]\n",
            "Saving to: ‘svhn.mat’\n",
            "\n",
            "svhn.mat            100%[===================>] 173.61M  51.0MB/s    in 4.4s    \n",
            "\n",
            "2025-05-08 17:27:57 (39.6 MB/s) - ‘svhn.mat’ saved [182040794/182040794]\n",
            "\n",
            "--2025-05-08 17:27:57--  https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/flowers/102/102flowers.tgz [following]\n",
            "--2025-05-08 17:27:58--  https://thor.robots.ox.ac.uk/flowers/102/102flowers.tgz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 344862509 (329M) [application/octet-stream]\n",
            "Saving to: ‘flowers102.tgz’\n",
            "\n",
            "flowers102.tgz      100%[===================>] 328.89M  20.9MB/s    in 24s     \n",
            "\n",
            "2025-05-08 17:28:22 (13.8 MB/s) - ‘flowers102.tgz’ saved [344862509/344862509]\n",
            "\n",
            "--2025-05-08 17:28:22--  https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6488666 (6.2M) [text/plain]\n",
            "Saving to: ‘big.txt’\n",
            "\n",
            "big.txt             100%[===================>]   6.19M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-08 17:28:23 (53.1 MB/s) - ‘big.txt’ saved [6488666/6488666]\n",
            "\n",
            "--2025-05-08 17:28:23--  https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4862985 (4.6M) [text/plain]\n",
            "Saving to: ‘words.txt’\n",
            "\n",
            "words.txt           100%[===================>]   4.64M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-08 17:28:23 (40.9 MB/s) - ‘words.txt’ saved [4862985/4862985]\n",
            "\n",
            "--2025-05-08 17:28:23--  https://raw.githubusercontent.com/dair-ai/emotion_dataset/master/data/text_emotion.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-05-08 17:28:23 ERROR 404: Not Found.\n",
            "\n",
            "--2025-05-08 17:28:23--  https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.53\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 487770 (476K) [application/x-gzip]\n",
            "Saving to: ‘rt_reviews.tar.gz’\n",
            "\n",
            "rt_reviews.tar.gz   100%[===================>] 476.34K  1.70MB/s    in 0.3s    \n",
            "\n",
            "2025-05-08 17:28:24 (1.70 MB/s) - ‘rt_reviews.tar.gz’ saved [487770/487770]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN (Text/Sequential) Datasets\n",
        "!wget -O airline.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\n",
        "!wget -O daily_temp.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\n",
        "!wget -O sunspots.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv\n",
        "!wget -O shampoo.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv\n",
        "!wget -O power.zip https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
        "!wget -O student.zip https://archive.ics.uci.edu/ml/machine-learning-databases/00356/student.zip\n",
        "!wget -O energy.csv https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\n",
        "!wget -O simple_time_series.csv https://raw.githubusercontent.com/selva86/datasets/master/electricity_consumption.csv\n",
        "!wget -O character_sequences.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!wget -O sunspots.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv\n",
        "!wget -O daily_births.csv https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-total-female-births.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XMHRrjBc2Von",
        "outputId": "07359a16-f60a-4055-d256-b852eb77f225"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-08 17:28:46--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2180 (2.1K) [text/plain]\n",
            "Saving to: ‘airline.csv’\n",
            "\n",
            "\rairline.csv           0%[                    ]       0  --.-KB/s               \rairline.csv         100%[===================>]   2.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-08 17:28:47 (21.4 MB/s) - ‘airline.csv’ saved [2180/2180]\n",
            "\n",
            "--2025-05-08 17:28:47--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67921 (66K) [text/plain]\n",
            "Saving to: ‘daily_temp.csv’\n",
            "\n",
            "daily_temp.csv      100%[===================>]  66.33K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-08 17:28:47 (2.03 MB/s) - ‘daily_temp.csv’ saved [67921/67921]\n",
            "\n",
            "--2025-05-08 17:28:47--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45039 (44K) [text/plain]\n",
            "Saving to: ‘sunspots.csv’\n",
            "\n",
            "sunspots.csv        100%[===================>]  43.98K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-05-08 17:28:47 (2.51 MB/s) - ‘sunspots.csv’ saved [45039/45039]\n",
            "\n",
            "--2025-05-08 17:28:47--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 519 [text/plain]\n",
            "Saving to: ‘shampoo.csv’\n",
            "\n",
            "shampoo.csv         100%[===================>]     519  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-08 17:28:47 (37.6 MB/s) - ‘shampoo.csv’ saved [519/519]\n",
            "\n",
            "--2025-05-08 17:28:47--  https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘power.zip’\n",
            "\n",
            "power.zip               [  <=>               ]  19.68M  67.2MB/s    in 0.3s    \n",
            "\n",
            "2025-05-08 17:28:48 (67.2 MB/s) - ‘power.zip’ saved [20640916]\n",
            "\n",
            "--2025-05-08 17:28:48--  https://archive.ics.uci.edu/ml/machine-learning-databases/00356/student.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘student.zip’\n",
            "\n",
            "student.zip             [ <=>                ]  20.00K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-05-08 17:28:48 (1.15 MB/s) - ‘student.zip’ saved [20478]\n",
            "\n",
            "--2025-05-08 17:28:48--  https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘energy.csv’\n",
            "\n",
            "energy.csv              [  <=>               ]  11.42M  32.4MB/s    in 0.4s    \n",
            "\n",
            "2025-05-08 17:28:49 (32.4 MB/s) - ‘energy.csv’ saved [11979363]\n",
            "\n",
            "--2025-05-08 17:28:49--  https://raw.githubusercontent.com/selva86/datasets/master/electricity_consumption.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-05-08 17:28:49 ERROR 404: Not Found.\n",
            "\n",
            "--2025-05-08 17:28:49--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘character_sequences.txt’\n",
            "\n",
            "character_sequences 100%[===================>]   1.06M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-05-08 17:28:49 (12.4 MB/s) - ‘character_sequences.txt’ saved [1115394/1115394]\n",
            "\n",
            "--2025-05-08 17:28:49--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45039 (44K) [text/plain]\n",
            "Saving to: ‘sunspots.csv’\n",
            "\n",
            "sunspots.csv        100%[===================>]  43.98K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-05-08 17:28:50 (2.77 MB/s) - ‘sunspots.csv’ saved [45039/45039]\n",
            "\n",
            "--2025-05-08 17:28:50--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-total-female-births.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6220 (6.1K) [text/plain]\n",
            "Saving to: ‘daily_births.csv’\n",
            "\n",
            "daily_births.csv    100%[===================>]   6.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-08 17:28:50 (61.0 MB/s) - ‘daily_births.csv’ saved [6220/6220]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction Commands for Special File Types\n",
        "\n",
        "# .npz\n",
        "!mkdir -p mnist_extracted\n",
        "!mv mnist.npz mnist_extracted/\n",
        "# (mnist.npz is directly loadable with numpy, no explicit unzip needed)\n",
        "\n",
        "# .gz\n",
        "!gunzip fashion-mnist-images.gz\n",
        "\n",
        "# .tar.gz and .tgz\n",
        "!mkdir -p cifar10_extracted\n",
        "!tar -xzf cifar10.tar.gz -C cifar10_extracted\n",
        "!mkdir -p cifar100_extracted\n",
        "!tar -xzf cifar100.tar.gz -C cifar100_extracted\n",
        "!mkdir -p flowers102_extracted\n",
        "!tar -xzf flowers102.tgz -C flowers102_extracted\n",
        "!mkdir -p rt_reviews_extracted\n",
        "!tar -xzf rt_reviews.tar.gz -C rt_reviews_extracted\n",
        "\n",
        "# .mat\n",
        "# (svhn.mat is a MATLAB file, loadable with libraries like scipy.io, no direct unzip/tar needed)\n",
        "\n",
        "# .zip\n",
        "!mkdir -p power_extracted\n",
        "!unzip power.zip -d power_extracted\n",
        "!mkdir -p student_extracted\n",
        "!unzip student.zip -d student_extracte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VuXRmfvk-xk2",
        "outputId": "5a7ec8e7-65d2-4342-d587-f0ad962ecd75"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  power.zip\n",
            "  inflating: power_extracted/household_power_consumption.txt  \n",
            "Archive:  student.zip\n",
            "  inflating: student_extracte/student-mat.csv  \n",
            "  inflating: student_extracte/student-por.csv  \n",
            "  inflating: student_extracte/student-merge.R  \n",
            "  inflating: student_extracte/student.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all first\n",
        "!mkdir -p rt_reviews_extracted && tar -xzf rt_reviews.tar.gz -C rt_reviews_extracted\n",
        "!mkdir -p student_extracted && unzip student.zip -d student_extracted\n",
        "\n",
        "# Then move one specific file you want\n",
        "!cp rt_reviews_extracted/rt-polaritydata/rt-polarity.neg rt_reviews.csv\n",
        "!cp student_extracted/student-mat.csv student.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOOtOTtPHjMY",
        "outputId": "90c5d72d-1d68-40d6-997a-cd1429b0ecb0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  student.zip\n",
            "  inflating: student_extracted/student-mat.csv  \n",
            "  inflating: student_extracted/student-por.csv  \n",
            "  inflating: student_extracted/student-merge.R  \n",
            "  inflating: student_extracted/student.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import gzip\n",
        "import zipfile\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis\n",
        "from PIL import Image\n",
        "\n",
        "# Ensure extraction directories exist\n",
        "os.makedirs('mnist_extracted', exist_ok=True)\n",
        "os.makedirs('cifar10_extracted', exist_ok=True)\n",
        "os.makedirs('cifar100_extracted', exist_ok=True)\n",
        "os.makedirs('flowers102_extracted', exist_ok=True)\n",
        "os.makedirs('rt_reviews_extracted', exist_ok=True)\n",
        "os.makedirs('power_extracted', exist_ok=True)\n",
        "os.makedirs('student_extracted', exist_ok=True)\n",
        "\n",
        "# Move .npz file\n",
        "if os.path.exists('mnist.npz'):\n",
        "    os.rename('mnist.npz', 'mnist_extracted/mnist.npz')\n",
        "\n",
        "# Unzip .gz\n",
        "if os.path.exists('fashion-mnist-images.gz'):\n",
        "    with gzip.open('fashion-mnist-images.gz', 'rb') as f_in:\n",
        "        with open('fashion-mnist-images', 'wb') as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "    os.remove('fashion-mnist-images.gz')\n",
        "\n",
        "# Untar .tar.gz and .tgz\n",
        "for tar_file, extract_dir in [\n",
        "    ('cifar10.tar.gz', 'cifar10_extracted'),\n",
        "    ('cifar100.tar.gz', 'cifar100_extracted'),\n",
        "    ('flowers102.tgz', 'flowers102_extracted'),\n",
        "    ('rt_reviews.tar.gz', 'rt_reviews_extracted')\n",
        "]:\n",
        "    if os.path.exists(tar_file):\n",
        "        try:\n",
        "            with tarfile.open(tar_file, 'r:gz') as tar:\n",
        "                tar.extractall(extract_dir)\n",
        "            os.remove(tar_file)\n",
        "        except tarfile.ReadError as e:\n",
        "            print(f\"Error extracting {tar_file}: {e}\")\n",
        "\n",
        "# Unzip .zip\n",
        "for zip_file, extract_dir in [\n",
        "    ('power.zip', 'power_extracted'),\n",
        "    ('student.zip', 'student_extracted')\n",
        "]:\n",
        "    if os.path.exists(zip_file):\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_dir)\n",
        "            os.remove(zip_file)\n",
        "        except zipfile.BadZipFile as e:\n",
        "            print(f\"Error extracting {zip_file}: {e}\")\n",
        "\n",
        "def compute_meta_features(df):\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    features = {\n",
        "        'n_instances': df.shape[0],\n",
        "        'n_features': df.shape[1],\n",
        "        'n_numeric_features': numeric_df.shape[1],\n",
        "        'n_categorical_features': df.shape[1] - numeric_df.shape[1],\n",
        "    }\n",
        "\n",
        "    if numeric_df.shape[1] > 0:\n",
        "        numeric_stats = numeric_df.describe()\n",
        "        features['mean_mean'] = numeric_stats.loc['mean'].mean()\n",
        "        features['mean_std'] = numeric_stats.loc['std'].mean()\n",
        "        features['mean_min'] = numeric_stats.loc['min'].mean()\n",
        "        features['mean_max'] = numeric_stats.loc['max'].mean()\n",
        "        features['mean_skew'] = skew(numeric_df.dropna().values.flatten(), nan_policy='omit')\n",
        "        features['mean_kurtosis'] = kurtosis(numeric_df.dropna().values.flatten(), nan_policy='omit')\n",
        "\n",
        "        # Outlier ratio using IQR\n",
        "        outlier_count = 0\n",
        "        total = 0\n",
        "        for col in numeric_df.columns:\n",
        "            Q1 = numeric_df[col].quantile(0.25)\n",
        "            Q3 = numeric_df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower = Q1 - 1.5 * IQR\n",
        "            upper = Q3 + 1.5 * IQR\n",
        "            outliers = numeric_df[(numeric_df[col] < lower) | (numeric_df[col] > upper)]\n",
        "            outlier_count += outliers.shape[0]\n",
        "            total += numeric_df[col].dropna().shape[0]\n",
        "        features['outlier_ratio'] = outlier_count / total if total > 0 else 0\n",
        "    else:\n",
        "        features.update({\n",
        "            k: np.nan for k in [\n",
        "                'mean_mean', 'mean_std', 'mean_min', 'mean_max',\n",
        "                'mean_skew', 'mean_kurtosis', 'outlier_ratio'\n",
        "            ]\n",
        "        })\n",
        "    return features\n",
        "\n",
        "def compute_binary_meta_features(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'rb') as f:\n",
        "            byte_data = f.read()\n",
        "        arr = np.frombuffer(byte_data, dtype=np.uint8)\n",
        "        entropy = -np.sum([p * np.log2(p) for p in np.bincount(arr, minlength=256) / len(arr) if p > 0]) if len(arr) > 0 else 0\n",
        "        return {\n",
        "            'n_instances': len(arr),\n",
        "            'n_features': 1,\n",
        "            'n_numeric_features': 1,\n",
        "            'n_categorical_features': 0,\n",
        "            'mean_mean': np.mean(arr) if len(arr) > 0 else np.nan,\n",
        "            'mean_std': np.std(arr) if len(arr) > 0 else np.nan,\n",
        "            'mean_min': np.min(arr) if len(arr) > 0 else np.nan,\n",
        "            'mean_max': np.max(arr) if len(arr) > 0 else np.nan,\n",
        "            'mean_skew': skew(arr) if len(arr) > 1 else np.nan,\n",
        "            'mean_kurtosis': kurtosis(arr) if len(arr) > 2 else np.nan,\n",
        "            'outlier_ratio': np.nan,\n",
        "            'entropy': entropy\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading binary file {filepath}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def compute_image_meta_features(filepath):\n",
        "    try:\n",
        "        img = Image.open(filepath)\n",
        "        img_array = np.array(img)\n",
        "        channels = 1 if len(img_array.shape) == 2 else img_array.shape[2]\n",
        "        return {\n",
        "            'img_height': img_array.shape[0],\n",
        "            'img_width': img_array.shape[1],\n",
        "            'img_channels': channels,\n",
        "            'mean_pixel_value': np.mean(img_array),\n",
        "            'std_pixel_value': np.std(img_array),\n",
        "            'min_pixel_value': np.min(img_array),\n",
        "            'max_pixel_value': np.max(img_array)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading image file {filepath}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def compute_image_dataset_meta_features(dirpath):\n",
        "    widths = []\n",
        "    heights = []\n",
        "    channels = []\n",
        "    formats = set()\n",
        "    n_images = 0\n",
        "    for filename in os.listdir(dirpath):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
        "            filepath = os.path.join(dirpath, filename)\n",
        "            try:\n",
        "                img = Image.open(filepath)\n",
        "                img_array = np.array(img)\n",
        "                heights.append(img_array.shape[0])\n",
        "                widths.append(img_array.shape[1])\n",
        "                channels.append(1 if len(img_array.shape) == 2 else img_array.shape[2])\n",
        "                formats.add(img.format)\n",
        "                n_images += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading image {filename} in {dirpath}: {e}\")\n",
        "    return {\n",
        "        'avg_img_width': np.mean(widths) if widths else np.nan,\n",
        "        'std_img_width': np.std(widths) if widths else np.nan,\n",
        "        'avg_img_height': np.mean(heights) if heights else np.nan,\n",
        "        'std_img_height': np.std(heights) if heights else np.nan,\n",
        "        'avg_img_channels': np.mean(channels) if channels else np.nan,\n",
        "        'unique_img_formats': list(formats),\n",
        "        'n_images': n_images\n",
        "    }\n",
        "\n",
        "def load_mnist_meta(filepath):\n",
        "    try:\n",
        "        with np.load(filepath) as f:\n",
        "            images = f['x_train']  # Assuming 'x_train' contains the images\n",
        "            return {\n",
        "                'n_instances': images.shape[0],\n",
        "                'img_height': images.shape[1],\n",
        "                'img_width': images.shape[2],\n",
        "                'img_channels': 1 if len(images.shape) == 3 else images.shape[3],\n",
        "                'pixel_mean': np.mean(images),\n",
        "                'pixel_std': np.std(images)\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading MNIST meta from {filepath}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def compute_cifar_meta(filepath):\n",
        "    try:\n",
        "        import pickle\n",
        "        with open(filepath, 'rb') as fo:\n",
        "            batch = pickle.load(fo, encoding='bytes')\n",
        "        data = batch[b'data']\n",
        "        labels = batch[b'labels']\n",
        "        return {\n",
        "            'n_instances': len(labels),\n",
        "            'img_height': 32,\n",
        "            'img_width': 32,\n",
        "            'img_channels': 3,\n",
        "            'pixel_mean': np.mean(data),\n",
        "            'pixel_std': np.std(data),\n",
        "            'n_classes': len(np.unique(labels))\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CIFAR meta from {filepath}: {e}\")\n",
        "        return {}\n",
        "\n",
        "def load_dataset(filepath):\n",
        "    if filepath.endswith('.csv'):\n",
        "        return pd.read_csv(filepath)\n",
        "    elif filepath.endswith('.mat'):\n",
        "        try:\n",
        "            mat_data = scipy.io.loadmat(filepath)\n",
        "            if 'data' in mat_data:\n",
        "                return pd.DataFrame(mat_data['data'])\n",
        "            elif 'X' in mat_data or 'images' in mat_data:\n",
        "                arr = mat_data.get('X', mat_data.get('images'))\n",
        "                if arr.ndim == 4:\n",
        "                    n_samples = arr.shape[3]\n",
        "                    arr_reshaped = arr.transpose(3, 2, 0, 1).reshape(n_samples, -1)\n",
        "                    return pd.DataFrame(arr_reshaped)\n",
        "                else:\n",
        "                    return pd.DataFrame(arr.reshape(arr.shape[0], -1))\n",
        "            else:\n",
        "                print(f\"Warning: Could not find a standard data key in {filepath}\")\n",
        "                return pd.DataFrame()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading .mat file {filepath}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "    elif filepath.endswith('.npz'):\n",
        "        try:\n",
        "            return pd.DataFrame(np.load(filepath)['arr_0'])  # Adjust key if needed\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading .npz file {filepath}: {e}\")\n",
        "            return pd.DataFrame()\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file type: {filepath}\")\n",
        "\n",
        "dataset_model_mapping = {\n",
        "    # MLP (Tabular)\n",
        "    'winequality-red.csv': 'MLP',\n",
        "    'abalone.csv': 'MLP',\n",
        "    'iris.csv': 'MLP',\n",
        "    'pima.csv': 'MLP',\n",
        "    'glass.csv': 'MLP',\n",
        "    'ionosphere.csv': 'MLP',\n",
        "    'haberman.csv': 'MLP',\n",
        "    'balance.csv': 'MLP',\n",
        "    'car.csv': 'MLP',\n",
        "    'seeds.csv': 'MLP',\n",
        "\n",
        "    # CNN (Image/Text)\n",
        "    'mnist_extracted/mnist.npz': 'CNN',\n",
        "    'fashion-mnist-images': 'CNN',\n",
        "    '/content/cifar10_extracted/cifar-10-batches-py': 'CNN',\n",
        "    '/content/cifar100_extracted/cifar-100-python/train': 'CNN',\n",
        "    'svhn.mat': 'CNN',\n",
        "    'flowers102_extracted/jpg': 'CNN',\n",
        "    'big.txt': 'CNN',\n",
        "    'words.txt': 'CNN',\n",
        "    '/content/rt_reviews_extracted/rt-polaritydata/rt-polarity.neg': 'CNN',\n",
        "\n",
        "    # RNN (only the datasets already present in your code)\n",
        "    'airline.csv': 'RNN',\n",
        "    'daily_temp.csv': 'RNN',\n",
        "    'sunspots.csv': 'RNN',\n",
        "    'shampoo.csv': 'RNN',\n",
        "    'power_extracted/household_power_consumption.txt': 'RNN',\n",
        "    'student_extracted/student-mat.csv': 'RNN',\n",
        "    'energy.csv': 'RNN',\n",
        "    'character_sequences.txt': 'RNN',\n",
        "    'daily_births.csv': 'RNN',\n",
        "}\n",
        "\n",
        "meta_rows = []\n",
        "\n",
        "for file_path, model in dataset_model_mapping.items():\n",
        "    try:\n",
        "        print(f\"Processing: {file_path}\")\n",
        "        features = {}\n",
        "        if os.path.isfile(file_path):\n",
        "            if file_path.endswith(('.mat', '.npz')):\n",
        "                if 'mnist' in file_path:\n",
        "                    features = load_mnist_meta(file_path)\n",
        "                else:\n",
        "                    try:\n",
        "                        df = load_dataset(file_path)\n",
        "                        if not df.empty:\n",
        "                            features = compute_meta_features(df)\n",
        "                        else:\n",
        "                            print(f\"Warning: Loaded DataFrame is empty for {file_path}\")\n",
        "                    except ValueError as e:\n",
        "                        print(f\"Warning: Could not load as tabular data: {file_path} - {e}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing .mat or .npz file: {e}\")\n",
        "            elif file_path.endswith('.csv'):\n",
        "                try:\n",
        "                    df = load_dataset(file_path)\n",
        "                    features = compute_meta_features(df)\n",
        "                except ValueError as e:\n",
        "                    print(f\"Warning: Could not load as tabular data: {file_path} - {e}\")\n",
        "            elif file_path.endswith('.txt'):\n",
        "                if model.lower() == 'rnn' and file_path in ['character_sequences.txt', 'big.txt']:\n",
        "                    try:\n",
        "                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                            all_text = f.read()\n",
        "                        if all_text:\n",
        "                            words = all_text.split()\n",
        "                            sentences = all_text.split('.')\n",
        "                            features['n_instances'] = 1\n",
        "                            features['n_features'] = 1\n",
        "                            features['n_numeric_features'] = np.nan\n",
        "                            features['n_categorical_features'] = 1\n",
        "                            features['text_length'] = len(all_text)\n",
        "                            features['n_words'] = len(words)\n",
        "                            features['avg_word_length'] = np.mean([len(word) for word in words]) if words else np.nan\n",
        "                            features['vocabulary_size'] = len(set(words)) if words else 0\n",
        "                            features['n_sentences'] = len([s.strip() for s in sentences if s.strip()])\n",
        "                            features['avg_sentence_length'] = np.mean([len(s.split()) for s in sentences if s.strip()]) if [s.strip() for s in sentences if s.strip()] else np.nan\n",
        "                            features['n_paragraphs'] = all_text.count('\\n\\n') + 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error reading text file {file_path} for RNN features: {e}\")\n",
        "                elif model.lower() == 'cnn' and file_path in ['words.txt', 'big.txt']:\n",
        "                    try:\n",
        "                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                            all_text = f.read()\n",
        "                        if file_path == 'words.txt':\n",
        "                            words_with_numbers = [line.strip() for line in all_text.splitlines() if line.strip()]\n",
        "                            words = [line.split(' ', 1)[1] if len(line.split(' ', 1)) > 1 else line for line in words_with_numbers]\n",
        "                            numbers = [line.split(' ', 1)[0] if len(line.split(' ', 1)) > 1 and line.split(' ', 1)[0].isdigit() else np.nan for line in words_with_numbers]\n",
        "                            valid_words = [w for w in words if isinstance(w, str)]\n",
        "                            features['n_instances'] = len(valid_words) if valid_words else 1\n",
        "                            features['n_features'] = 1\n",
        "                            features['n_numeric_features'] = np.nan\n",
        "                            features['n_categorical_features'] = 1\n",
        "                            features['n_words_listed'] = len(valid_words)\n",
        "                            features['avg_word_length'] = np.mean([len(word) for word in valid_words]) if valid_words else np.nan\n",
        "                            features['n_uniquewords'] = len(set(valid_words)) if valid_words else 0\n",
        "                            features['n_numbered_entries'] = sum(1 for num in numbers if pd.notna(num))\n",
        "                            if all(pd.notna(num) for num in numbers):\n",
        "                                numbers = [int(n) for n in numbers]\n",
        "                                features['number_sequence_monotonic'] = all(numbers[i] <= numbers[i+1] for i in range(len(numbers)-1)) if len(numbers) > 1 else np.nan\n",
        "                            else:\n",
        "                                features['number_sequence_monotonic'] = np.nan\n",
        "                        else:\n",
        "                            words = [line.strip() for line in all_text.splitlines() if line.strip()]\n",
        "                            features['n_instances'] = len(words)\n",
        "                            features['n_features'] = 1\n",
        "                            features['n_numeric_features'] = np.nan\n",
        "                            features['n_categorical_features'] = 1\n",
        "                            features['n_words'] = len(words)\n",
        "                            features['avg_word_length'] = np.mean([len(word) for word in words]) if words else np.nan\n",
        "                            features['n_uniquewords'] = len(set(words)) if words else 0\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error reading text file {file_path} for CNN features: {e}\")\n",
        "                else:\n",
        "                    features = compute_binary_meta_features(file_path)\n",
        "            elif file_path.endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
        "                features = compute_image_meta_features(file_path)\n",
        "            else:\n",
        "                features = compute_binary_meta_features(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "            if model == 'CNN':\n",
        "                features = compute_image_dataset_meta_features(file_path)\n",
        "            else:\n",
        "                print(f\"Warning: Directory found for non-image dataset: {file_path}\")\n",
        "        else:\n",
        "            print(f\"Warning: File or directory not found: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        features['dataset'] = file_path\n",
        "        features['model_type'] = model\n",
        "        meta_rows.append(features)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "# Check for missing CNN and RNN datasets\n",
        "cnn_datasets = [k for k, v in dataset_model_mapping.items() if v == 'CNN']\n",
        "rnn_datasets = [k for k, v in dataset_model_mapping.items() if v == 'RNN']\n",
        "meta_df = pd.DataFrame(meta_rows)\n",
        "processed_datasets = set(meta_df['dataset'])\n",
        "missing_cnn = [ds for ds in cnn_datasets if ds not in processed_datasets]\n",
        "missing_rnn = [ds for ds in rnn_datasets if ds not in processed_datasets]\n",
        "print(\"Missing CNN datasets:\", missing_cnn)\n",
        "print(\"Missing RNN datasets:\", missing_rnn)\n",
        "\n",
        "# Save meta-features to CSV\n",
        "meta_df.to_csv('dataset_meta_features.csv', index=False)\n",
        "print(\"Meta-features saved to dataset_meta_features.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7vA-qgCKaua",
        "outputId": "2b6ecf3a-4983-4e72-eb75-bceb3faa4d63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: winequality-red.csv\n",
            "Processing: abalone.csv\n",
            "Processing: iris.csv\n",
            "Processing: pima.csv\n",
            "Processing: glass.csv\n",
            "Processing: ionosphere.csv\n",
            "Processing: haberman.csv\n",
            "Processing: balance.csv\n",
            "Processing: car.csv\n",
            "Processing: seeds.csv\n",
            "Processing: mnist_extracted/mnist.npz\n",
            "Processing: fashion-mnist-images\n",
            "Processing: /content/cifar10_extracted/cifar-10-batches-py\n",
            "Processing: /content/cifar100_extracted/cifar-100-python/train\n",
            "Processing: svhn.mat\n",
            "Processing: flowers102_extracted/jpg\n",
            "Processing: big.txt\n",
            "Processing: words.txt\n",
            "Processing: /content/rt_reviews_extracted/rt-polaritydata/rt-polarity.neg\n",
            "Processing: airline.csv\n",
            "Processing: daily_temp.csv\n",
            "Processing: sunspots.csv\n",
            "Processing: shampoo.csv\n",
            "Processing: power_extracted/household_power_consumption.txt\n",
            "Processing: student_extracted/student-mat.csv\n",
            "Processing: energy.csv\n",
            "Processing: character_sequences.txt\n",
            "Processing: daily_births.csv\n",
            "Missing CNN datasets: []\n",
            "Missing RNN datasets: []\n",
            "Meta-features saved to dataset_meta_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "lOfTGpEREBZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import joblib\n",
        "\n",
        "# Load your meta-features\n",
        "meta_df = pd.read_csv('dataset_meta_features.csv')\n",
        "drop_cols = ['dataset', 'model_type']\n",
        "\n",
        "# Drop non-numeric columns except for the target and dataset\n",
        "non_numeric_cols = meta_df.select_dtypes(include=['object']).columns.tolist()\n",
        "non_numeric_cols = [col for col in non_numeric_cols if col not in drop_cols]\n",
        "X = meta_df.drop(columns=drop_cols + non_numeric_cols)\n",
        "\n",
        "y = meta_df['model_type']\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Train the model (Random Forest)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_imputed, y_encoded)\n",
        "\n",
        "# Evaluate\n",
        "scores = cross_val_score(clf, X_imputed, y_encoded, cv=5)\n",
        "print(f\"Cross-validated accuracy: {scores.mean():.2f} (+/- {scores.std():.2f})\")\n",
        "\n",
        "# Save model, imputer, and label encoder\n",
        "joblib.dump(clf, 'meta_model.pkl')\n",
        "joblib.dump(imputer, 'meta_imputer.pkl')\n",
        "joblib.dump(le, 'meta_label_encoder.pkl')\n",
        "print(\"Model, imputer, and label encoder saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBhtxBpv88Gx",
        "outputId": "ec111075-239a-4d25-a252-2cf258f0aed1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy: 0.67 (+/- 0.19)\n",
            "Model, imputer, and label encoder saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "SwpiXcfAD2BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import joblib\n",
        "from scipy.stats import skew, kurtosis\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ---- Your meta-feature extraction functions (tabular only shown here; add others as needed) ----\n",
        "\n",
        "def compute_meta_features(df):\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    features = {\n",
        "        'n_instances': df.shape[0],\n",
        "        'n_features': df.shape[1],\n",
        "        'n_numeric_features': numeric_df.shape[1],\n",
        "        'n_categorical_features': df.shape[1] - numeric_df.shape[1],\n",
        "    }\n",
        "    if numeric_df.shape[1] > 0:\n",
        "        numeric_stats = numeric_df.describe()\n",
        "        features['mean_mean'] = numeric_stats.loc['mean'].mean()\n",
        "        features['mean_std'] = numeric_stats.loc['std'].mean()\n",
        "        features['mean_min'] = numeric_stats.loc['min'].mean()\n",
        "        features['mean_max'] = numeric_stats.loc['max'].mean()\n",
        "        features['mean_skew'] = skew(numeric_df.dropna().values.flatten(), nan_policy='omit')\n",
        "        features['mean_kurtosis'] = kurtosis(numeric_df.dropna().values.flatten(), nan_policy='omit')\n",
        "        # Outlier ratio using IQR\n",
        "        outlier_count = 0\n",
        "        total = 0\n",
        "        for col in numeric_df.columns:\n",
        "            Q1 = numeric_df[col].quantile(0.25)\n",
        "            Q3 = numeric_df[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower = Q1 - 1.5 * IQR\n",
        "            upper = Q3 + 1.5 * IQR\n",
        "            outliers = numeric_df[(numeric_df[col] < lower) | (numeric_df[col] > upper)]\n",
        "            outlier_count += outliers.shape[0]\n",
        "            total += numeric_df[col].dropna().shape[0]\n",
        "        features['outlier_ratio'] = outlier_count / total if total > 0 else 0\n",
        "    else:\n",
        "        features.update({\n",
        "            k: np.nan for k in [\n",
        "                'mean_mean', 'mean_std', 'mean_min', 'mean_max',\n",
        "                'mean_skew', 'mean_kurtosis', 'outlier_ratio'\n",
        "            ]\n",
        "        })\n",
        "    return features\n",
        "\n",
        "def compute_image_meta_features(filepath):\n",
        "    from PIL import Image\n",
        "    img = Image.open(filepath)\n",
        "    img_array = np.array(img)\n",
        "    channels = 1 if len(img_array.shape) == 2 else img_array.shape[2]\n",
        "    return {\n",
        "        'img_height': img_array.shape[0],\n",
        "        'img_width': img_array.shape[1],\n",
        "        'img_channels': channels,\n",
        "        'mean_pixel_value': np.mean(img_array),\n",
        "        'std_pixel_value': np.std(img_array),\n",
        "        'min_pixel_value': np.min(img_array),\n",
        "        'max_pixel_value': np.max(img_array)\n",
        "    }\n",
        "\n",
        "def compute_text_meta_features(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        all_text = f.read()\n",
        "    words = all_text.split()\n",
        "    sentences = all_text.split('.')\n",
        "    return {\n",
        "        'n_instances': 1,\n",
        "        'n_features': 1,\n",
        "        'n_numeric_features': np.nan,\n",
        "        'n_categorical_features': 1,\n",
        "        'text_length': len(all_text),\n",
        "        'n_words': len(words),\n",
        "        'avg_word_length': np.mean([len(word) for word in words]) if words else np.nan,\n",
        "        'vocabulary_size': len(set(words)) if words else 0,\n",
        "        'n_sentences': len([s.strip() for s in sentences if s.strip()]),\n",
        "        'avg_sentence_length': np.mean([len(s.split()) for s in sentences if s.strip()]) if [s.strip() for s in sentences if s.strip()] else np.nan,\n",
        "        'n_paragraphs': all_text.count('\\n\\n') + 1,\n",
        "    }\n",
        "\n",
        "# ---- Download helpers ----\n",
        "\n",
        "def download_file(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    return filename\n",
        "\n",
        "# ---- Load meta-model and preprocessors ----\n",
        "\n",
        "clf = joblib.load('meta_model.pkl')\n",
        "imputer = joblib.load('meta_imputer.pkl')\n",
        "le = joblib.load('meta_label_encoder.pkl')\n",
        "meta_df = pd.read_csv('dataset_meta_features.csv')\n",
        "drop_cols = ['dataset', 'model_type']\n",
        "non_numeric_cols = meta_df.select_dtypes(include=['object']).columns.tolist()\n",
        "non_numeric_cols = [col for col in non_numeric_cols if col not in drop_cols]\n",
        "feature_cols = [col for col in meta_df.columns if col not in drop_cols + non_numeric_cols]\n",
        "\n",
        "def recommend_model(meta_features_dict):\n",
        "    row = [meta_features_dict.get(col, np.nan) for col in feature_cols]\n",
        "    row_imputed = imputer.transform([row])\n",
        "    pred = clf.predict(row_imputed)\n",
        "    return le.inverse_transform(pred)[0]\n",
        "\n",
        "# ---- 20 Datasets (url, local filename, type, ground truth) ----\n",
        "\n",
        "datasets = [\n",
        "    # Tabular\n",
        "    (\"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", \"wdbc.data\", \"tabular\", \"MLP\"),\n",
        "    (\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\", \"titanic.csv\", \"tabular\", \"MLP\"),\n",
        "    (\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", \"adult.data\", \"tabular\", \"MLP\"),\n",
        "    (\"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip\", \"bank.zip\", \"tabular\", \"MLP\"),\n",
        "    (\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", \"winequality-white.csv\", \"tabular\", \"MLP\"),\n",
        "    (\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", \"iris.data\", \"tabular\", \"MLP\"),\n",
        "    (\"https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\", \"energydata_complete.csv\", \"tabular\", \"MLP\"),\n",
        "    # Images\n",
        "    (\"https://github.com/myleott/mnist_png/raw/master/mnist_png/testing/0/1.png\", \"mnist1.png\", \"image\", \"CNN\"),\n",
        "    (\"https://github.com/zalandoresearch/fashion-mnist/raw/master/img/img_0.jpg\", \"fashionmnist0.jpg\", \"image\", \"CNN\"),\n",
        "    (\"https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog1.png\", \"cifar10dog.png\", \"image\", \"CNN\"),\n",
        "    (\"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images/Abyssinian_1.jpg\", \"pet1.jpg\", \"image\", \"CNN\"),\n",
        "    (\"https://cs.stanford.edu/~acoates/stl10/stl10_binary/unlabeled/unlabeled_0.png\", \"stl10_0.png\", \"image\", \"CNN\"),\n",
        "    (\"https://www.cs.toronto.edu/~kriz/cifar-100-sample/butterfly1.png\", \"cifar100butterfly.png\", \"image\", \"CNN\"),\n",
        "    # Text / Time-series\n",
        "    (\"https://www.gutenberg.org/files/1041/1041-0.txt\", \"sonnets.txt\", \"text\", \"RNN\"),\n",
        "    (\"https://www.gutenberg.org/files/11/11-0.txt\", \"alice.txt\", \"text\", \"RNN\"),\n",
        "    (\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\", \"daily-min-temperatures.csv\", \"tabular\", \"RNN\"),\n",
        "    (\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv\", \"monthly-sunspots.csv\", \"tabular\", \"RNN\"),\n",
        "    (\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\", \"airline-passengers.csv\", \"tabular\", \"RNN\"),\n",
        "    (\"https://www.gutenberg.org/files/1524/1524-0.txt\", \"hamlet.txt\", \"text\", \"RNN\"),\n",
        "    (\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \"aclImdb_v1.tar.gz\", \"text\", \"RNN\"),\n",
        "]\n",
        "\n",
        "# ---- Test loop ----\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for url, fname, dtype, true_model in datasets:\n",
        "    print(f\"\\nProcessing {fname} ({dtype}) ...\")\n",
        "    try:\n",
        "        # Download and extract if needed\n",
        "        if dtype == \"tabular\":\n",
        "            if fname.endswith('.zip'):\n",
        "                import zipfile\n",
        "                download_file(url, fname)\n",
        "                with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(\"bank_data\")\n",
        "                csv_path = os.path.join(\"bank_data\", \"bank-full.csv\")\n",
        "                df = pd.read_csv(csv_path, sep=';')\n",
        "            elif fname.endswith('.csv') or fname.endswith('.data'):\n",
        "                download_file(url, fname)\n",
        "                if fname == \"adult.data\":\n",
        "                    df = pd.read_csv(fname, header=None, skipinitialspace=True)\n",
        "                else:\n",
        "                    df = pd.read_csv(fname)\n",
        "                # For iris.data, remove empty rows if any\n",
        "                if fname == \"iris.data\":\n",
        "                    df = df.dropna()\n",
        "            else:\n",
        "                continue\n",
        "            meta_features = compute_meta_features(df)\n",
        "        elif dtype == \"image\":\n",
        "            download_file(url, fname)\n",
        "            meta_features = compute_image_meta_features(fname)\n",
        "        elif dtype == \"text\":\n",
        "            if fname.endswith('.tar.gz'):\n",
        "                import tarfile\n",
        "                download_file(url, fname)\n",
        "                with tarfile.open(fname, 'r:gz') as tar:\n",
        "                    tar.extractall(\"aclImdb\")\n",
        "                # Use a small sample file for testing\n",
        "                txt_path = os.path.join(\"aclImdb\", \"aclImdb\", \"test\", \"pos\", \"0_10.txt\")\n",
        "                if not os.path.exists(txt_path):\n",
        "                    # fallback: just pick any .txt in extracted dir\n",
        "                    for root, dirs, files in os.walk(\"aclImdb\"):\n",
        "                        for file in files:\n",
        "                            if file.endswith('.txt'):\n",
        "                                txt_path = os.path.join(root, file)\n",
        "                                break\n",
        "                meta_features = compute_text_meta_features(txt_path)\n",
        "            else:\n",
        "                download_file(url, fname)\n",
        "                meta_features = compute_text_meta_features(fname)\n",
        "        else:\n",
        "            print(f\"Unknown data type: {dtype}\")\n",
        "            continue\n",
        "\n",
        "        predicted_model = recommend_model(meta_features)\n",
        "        print(f\"True: {true_model}, Predicted: {predicted_model}\")\n",
        "        y_true.append(true_model)\n",
        "        y_pred.append(predicted_model)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {fname}: {e}\")\n",
        "\n",
        "# ---- Compute and print accuracy ----\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\nMeta-model accuracy on 20 test datasets: {acc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NeWB-A7_2kw",
        "outputId": "40b75f1c-cf38-4853-8264-4b840c164143"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing wdbc.data (tabular) ...\n",
            "True: MLP, Predicted: RNN\n",
            "\n",
            "Processing titanic.csv (tabular) ...\n",
            "True: MLP, Predicted: MLP\n",
            "\n",
            "Processing adult.data (tabular) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: MLP, Predicted: RNN\n",
            "\n",
            "Processing bank.zip (tabular) ...\n",
            "True: MLP, Predicted: RNN\n",
            "\n",
            "Processing winequality-white.csv (tabular) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: MLP, Predicted: MLP\n",
            "\n",
            "Processing iris.data (tabular) ...\n",
            "True: MLP, Predicted: MLP\n",
            "\n",
            "Processing energydata_complete.csv (tabular) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True: MLP, Predicted: RNN\n",
            "\n",
            "Processing mnist1.png (image) ...\n",
            "Error processing mnist1.png: cannot identify image file 'mnist1.png'\n",
            "\n",
            "Processing fashionmnist0.jpg (image) ...\n",
            "Error processing fashionmnist0.jpg: cannot identify image file 'fashionmnist0.jpg'\n",
            "\n",
            "Processing cifar10dog.png (image) ...\n",
            "True: CNN, Predicted: CNN\n",
            "\n",
            "Processing pet1.jpg (image) ...\n",
            "Error processing pet1.jpg: cannot identify image file 'pet1.jpg'\n",
            "\n",
            "Processing stl10_0.png (image) ...\n",
            "Error processing stl10_0.png: cannot identify image file 'stl10_0.png'\n",
            "\n",
            "Processing cifar100butterfly.png (image) ...\n",
            "Error processing cifar100butterfly.png: cannot identify image file 'cifar100butterfly.png'\n",
            "\n",
            "Processing sonnets.txt (text) ...\n",
            "True: RNN, Predicted: RNN\n",
            "\n",
            "Processing alice.txt (text) ...\n",
            "True: RNN, Predicted: RNN\n",
            "\n",
            "Processing daily-min-temperatures.csv (tabular) ...\n",
            "True: RNN, Predicted: RNN\n",
            "\n",
            "Processing monthly-sunspots.csv (tabular) ...\n",
            "True: RNN, Predicted: RNN\n",
            "\n",
            "Processing airline-passengers.csv (tabular) ...\n",
            "True: RNN, Predicted: RNN\n",
            "\n",
            "Processing hamlet.txt (text) ...\n",
            "True: RNN, Predicted: RNN\n",
            "\n",
            "Processing aclImdb_v1.tar.gz (text) ...\n",
            "True: RNN, Predicted: RNN\n",
            "\n",
            "Meta-model accuracy on 20 test datasets: 0.73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['number_sequence_monotonic']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}